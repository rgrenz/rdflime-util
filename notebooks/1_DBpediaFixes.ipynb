{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBpedia Fixes\n",
    "The gold-standard datasets (metacritic-movies etc.) reference some entity URIs that do not exist anymore in our version of the DBpedia Knowledge Graph (subset of dump 2016-04).\n",
    "\n",
    "Example:\n",
    "http://dbpedia.org/resource/Carpool_(film) -> http://dbpedia.org/resource/Carpool_(1996_film)\n",
    "\n",
    "This notebook aims to identify and fix these issues. It outputs a fixed version of the corresponding dataset in the original dataset folder.\n",
    "\n",
    "Make sure that Jena Fuseki is up and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string: single '}' is not allowed (connectors.py, line 159)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m~/.cache/pypoetry/virtualenvs/rdflime-util-cLuY5sAo-py3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3398\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  Input \u001b[1;32mIn [1]\u001b[0m in \u001b[1;35m<cell line: 2>\u001b[0m\n    from pyrdf2vec.graphs import KG\n",
      "  File \u001b[1;32m~/.cache/pypoetry/virtualenvs/rdflime-util-cLuY5sAo-py3.8/lib/python3.8/site-packages/pyrdf2vec/__init__.py:3\u001b[0m in \u001b[1;35m<module>\u001b[0m\n    from .rdf2vec import RDF2VecTransformer\n",
      "  File \u001b[1;32m~/.cache/pypoetry/virtualenvs/rdflime-util-cLuY5sAo-py3.8/lib/python3.8/site-packages/pyrdf2vec/rdf2vec.py:11\u001b[0m in \u001b[1;35m<module>\u001b[0m\n    from pyrdf2vec.graphs import KG\n",
      "  File \u001b[1;32m~/.cache/pypoetry/virtualenvs/rdflime-util-cLuY5sAo-py3.8/lib/python3.8/site-packages/pyrdf2vec/graphs/__init__.py:1\u001b[0m in \u001b[1;35m<module>\u001b[0m\n    from .kg import KG\n",
      "\u001b[0;36m  File \u001b[0;32m~/.cache/pypoetry/virtualenvs/rdflime-util-cLuY5sAo-py3.8/lib/python3.8/site-packages/pyrdf2vec/graphs/kg.py:14\u001b[0;36m in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from pyrdf2vec.connectors import SPARQLConnector\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m~/.cache/pypoetry/virtualenvs/rdflime-util-cLuY5sAo-py3.8/lib/python3.8/site-packages/pyrdf2vec/connectors.py:159\u001b[0;36m\u001b[0m\n\u001b[0;31m    query += f\"{query_object} . }\"\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: single '}' is not allowed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyrdf2vec.graphs import KG\n",
    "import re\n",
    "import os.path\n",
    "from rdflimeConfig import dbpediaLocation, datasets, load_dataset\n",
    "import json\n",
    "\n",
    "# 0: Metacritic-Movies\n",
    "# 1: Metacritic-Albums\n",
    "# 2: Forbes-Companies\n",
    "# 3: Mercer-Cities\n",
    "# 4: AAUP-Universities\n",
    "# -> See rdflimeConfig.py\n",
    "explored_dataset_idx = 1\n",
    "dataset_full, dataset_entities = load_dataset(datasets[explored_dataset_idx], fixed=False)\n",
    "cfg = datasets[explored_dataset_idx]\n",
    "entity_kind = cfg[\"entity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify broken URIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 out of 1600 albums have broken URIs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.03125"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initiate dbpedia connection\n",
    "dbpedia = KG(dbpediaLocation)\n",
    "\n",
    "# Check gold-standard datasets\n",
    "missing = [entity for entity in dataset_entities if not dbpedia.is_exist([entity])]\n",
    "print(f\"{len(missing)} out of {len(dataset_entities)} {entity_kind} have broken URIs.\")\n",
    "len(missing)/len(dataset_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply URI fixes so that DBpedia and gold-standard dataset match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 can be fixed by using the fixed DBpedia_URI15 instead.\n"
     ]
    }
   ],
   "source": [
    "# Try using the (fixed) DBpedia_URI15\n",
    "ctr = len(fixes)\n",
    "for entity in missing:\n",
    "    fix = dataset_full[dataset_full[\"DBpedia_URI\"] == entity].DBpedia_URI15.values[0]\n",
    "    fix = fix.replace(\" \", \"+\") # Some URIs contain spaces instead of \"+\"\n",
    "    if dbpedia.is_exist([fix]): fixes.append({\"original\": entity, \"fix\": fix})\n",
    "\n",
    "print(f\"{len(fixes)-ctr} can be fixed by using the fixed DBpedia_URI15 instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset-specific fixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specific fixes for metacritic-movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 can be fixed by adding _(film) or _(yyyy_film) to the URI.\n"
     ]
    }
   ],
   "source": [
    "# Try adding _(film) or _(yyyy_film) to URI\n",
    "ctr = len(fixes)\n",
    "for movie in missing:\n",
    "    \n",
    "    # Simply add _(film)\n",
    "    simpleFix = movie + \"_(film)\"\n",
    "    if dbpedia.is_exist([simpleFix]) and movie not in [f[\"original\"] for f in fixes]: \n",
    "        fixes.append({\"original\": movie, \"fix\": simpleFix})\n",
    "    \n",
    "    # Try to add _(yyyy_film)\n",
    "    else:\n",
    "        releaseDate = dataset_full[dataset_full.DBpedia_URI == movie][\"Release date\"].iloc[0]\n",
    "        releaseYear = int(re.search(r\"\\d{4}\", releaseDate).group(0))\n",
    "\n",
    "        maxDistYears = 1\n",
    "        for y in range(releaseYear-maxDistYears, releaseYear+maxDistYears+1):\n",
    "\n",
    "            # _(movie) -> _(yyyy_movie) or {blank} -> _(yyyy_movie)\n",
    "            fix = movie.replace(\"(film)\", f\"({y}_film)\")\n",
    "            if not fix.endswith(\"film)\"):\n",
    "                fix += f\"_({y}_film)\"\n",
    "\n",
    "            if dbpedia.is_exist([fix]) and movie not in [f[\"original\"] for f in fixes]:\n",
    "                fixes.append({\"original\": movie, \"fix\": fix})\n",
    "                break\n",
    "print(f\"{len(fixes)-ctr} can be fixed by adding _(film) or _(yyyy_film) to the URI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 can be fixed by removing duplicate (film) part of URI.\n"
     ]
    }
   ],
   "source": [
    "# Try removing duplicate _(film) or _(yyyy_film) endings\n",
    "ctr = len(fixes)\n",
    "for movie in missing:\n",
    "    match = re.search(r\"(?:_\\((?:\\d{4}_|)film\\)){2}\", movie)\n",
    "    if match:\n",
    "        fix = match.group(0)\n",
    "        fix = fix[:int(len(fix)/2)]\n",
    "        fix = movie.replace(fix, \"\", 1)\n",
    "        if dbpedia.is_exist([fix]) and movie not in [f[\"original\"] for f in fixes]:\n",
    "            fixes.append({\"original\": movie, \"fix\": fix})\n",
    "print(f\"{len(fixes)-ctr} can be fixed by removing duplicate (film) part of URI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specific fixes for metacritic-albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 can be fixed by adding _(album).\n"
     ]
    }
   ],
   "source": [
    "# Try adding _(album)\n",
    "ctr = len(fixes)\n",
    "for album in missing:\n",
    "    for ending in [\"_(album)\"]:\n",
    "        fix = album + ending\n",
    "        if dbpedia.is_exist([fix]) and album not in [f[\"original\"] for f in fixes]:\n",
    "            fixes.append({\"original\": album, \"fix\": fix})\n",
    "print(f\"{len(fixes)-ctr} can be fixed by adding _(album).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specific fixes for forbes-companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 can be fixed by adding legal forms.\n"
     ]
    }
   ],
   "source": [
    "# Try various legal forms\n",
    "ctr = len(fixes)\n",
    "for company in missing:\n",
    "    for ending in [\"_Corporation\", \"_(company)\", \"_Group\", \"_AG\", \"_SE\", \"_Inc.\", \"_EMC\", \"_SA\", \"_ASA\", \"_N.V.\", \"_plc\"]:\n",
    "        fix = company + ending\n",
    "        if dbpedia.is_exist([fix]) and company not in [f[\"original\"] for f in fixes]:\n",
    "            fixes.append({\"original\": company, \"fix\": fix})\n",
    "print(f\"{len(fixes)-ctr} can be fixed by adding legal forms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specific fixes for aaup-salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 can be fixed by replacing URL encoded part.\n"
     ]
    }
   ],
   "source": [
    "# Replace URL encoded part\n",
    "ctr = len(fixes)\n",
    "for university in missing:\n",
    "    fix = university.replace(\"%E2%80%93\",\"â€“\")\n",
    "    if dbpedia.is_exist([fix]) and university not in [f[\"original\"] for f in fixes]:\n",
    "        fixes.append({\"original\": university, \"fix\": fix})\n",
    "print(f\"{len(fixes)-ctr} can be fixed by replacing URL encoded part.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 can be fixed manually.\n"
     ]
    }
   ],
   "source": [
    "ctr = len(fixes)\n",
    "\n",
    "file = os.path.join(cfg[\"location\"], \"manual_fixes.json\")\n",
    "with open(file, \"r\") as f:\n",
    "    manual_fixes = json.load(f)\n",
    "\n",
    "for fix in manual_fixes:\n",
    "    if fix[\"original\"] not in [f[\"original\"] for f in fixes]:\n",
    "        fixes.append(fix)\n",
    "\n",
    "print(f\"{len(fixes)-ctr} can be fixed manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "remaining = [m for m in missing if m not in [f[\"original\"] for f in fixes]]\n",
    "print(remaining)\n",
    "print(len(remaining))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and create fixed datsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_full[\"DBpedia_URI16\"] = dataset_full.DBpedia_URI\n",
    "\n",
    "for fix in fixes: \n",
    "    dataset_full[\"DBpedia_URI16\"] = dataset_full[\"DBpedia_URI16\"].replace(fix[\"original\"], fix[\"fix\"])\n",
    "    \n",
    "len(dataset_full[dataset_full.DBpedia_URI != dataset_full.DBpedia_URI16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fixed dataset and stored fixes to file.\n"
     ]
    }
   ],
   "source": [
    "# All issues fixed?\n",
    "still_missing = [entity for entity in dataset_full.DBpedia_URI16.values if not dbpedia.is_exist([entity])]\n",
    "assert len(still_missing) == 0, \"Not every broken URI has been fixed!\"\n",
    "\n",
    "# Duplicate entries make this check impossible: Concordia College - http://www.wikidata.org/entity/Q5158956\n",
    "# No duplicate fixes?\n",
    "# assert len(fixes) == len(set([fix[\"original\"] for fix in fixes])), \"Some entities have more than one fix!\"\n",
    "dataset_full.to_csv(os.path.join(cfg[\"location\"], \"data_fixed.tsv\"), sep=\"\\t\", index=False)\n",
    "\n",
    "print(\"Created fixed dataset and stored fixes to file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "still_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rdflime-util-cLuY5sAo-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce5a4af9a72a016c4887bc4fbd6c73d917f9c9f5d4de7f9550be99bde2950c61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
