{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from importlib import reload\n",
    "import lime.lime_rdf\n",
    "from rdflimeConfig import movieLocation\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "\n",
    "movieFull = pd.read_csv(os.path.join(movieLocation, \"movies_fixed.tsv\"), sep=\"\\t\")\n",
    "movies = [movie.DBpedia_URI for _, movie in movieFull.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(lime.lime_rdf)\n",
    "from lime.lime_rdf import LimeRdfExplainer\n",
    "\n",
    "with open(os.path.join(movieLocation, \"transformers\", \"rdf2vec_transformer_cbow_200\"), \"rb\") as file:\n",
    "    rdf2vec_transformer = pickle.load(file)\n",
    "\n",
    "with open(os.path.join(movieLocation, \"classifiers\", \"svc_100_cbow_200\"), \"rb\") as file:\n",
    "    clf = pickle.load(file)\n",
    "\n",
    "explainer = LimeRdfExplainer(\n",
    "    transformer=rdf2vec_transformer, \n",
    "    entities=movies,\n",
    "    class_names=clf.classes_,\n",
    "    kernel=None,\n",
    "    kernel_width=25,\n",
    "    verbose=False,\n",
    "    feature_selection=\"auto\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "explained_entity_id = 274  # 0-400 -> test data\n",
    "explained_entity_uri = movies[explained_entity_id]\n",
    "prediction = clf.predict_proba([rdf2vec_transformer._embeddings[explained_entity_id]])\n",
    "\n",
    "\n",
    "print(\"Explaining\", explained_entity_uri)\n",
    "print(\"Original prediction:\", prediction, \" / \".join(clf.classes_))\n",
    "print(\"True class:\", movieFull.iloc[explained_entity_id].label)\n",
    "\n",
    "data, probabilities, distances, explanation = explainer.explain_instance(\n",
    "    entity=explained_entity_uri, \n",
    "    classifier_fn=clf.predict_proba,\n",
    "    num_features=50,\n",
    "    num_samples=5000,\n",
    "    allow_triple_addition=False,\n",
    "    allow_triple_subtraction=True,\n",
    "    max_changed_triples=20,\n",
    "    change_count_fixed=True,\n",
    "    use_w2v_freeze=True,\n",
    "    center_correction=False,\n",
    "    single_run=False,\n",
    "    train_with_all=False,\n",
    "    distance_metric=\"cosine\",\n",
    "    model_regressor=None,\n",
    "    short_uris=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (4, .4*len(explanation.as_list()))\n",
    "\n",
    "explanation.domain_mapper.short_uris=True\n",
    "fig = explanation.as_pyplot_figure(figsize=figsize)\n",
    "explanation.domain_mapper.short_uris=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(probabilities).plot()\n",
    "\n",
    "# thesis: show plot with/without single_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.domain_mapper.short_uris=True\n",
    "with open(\"test.html\", \"w\") as f:\n",
    "    f.write(explanation.as_html())\n",
    "    explanation.domain_mapper.short_uris=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iw = explainer.indexed_walks\n",
    "\n",
    "top_triples = explanation.as_list()[:15]\n",
    "\n",
    "relevant_walks = []\n",
    "for triple in top_triples:\n",
    "    for w in iw.walks(explained_entity_uri, tuple(triple[0])):\n",
    "        \n",
    "        entity_pos = w.index(explained_entity_uri)\n",
    "\n",
    "        for i in range(2, len(w)):\n",
    "            if w[i-2] == triple[0][0] and w[i-1] == triple[0][1] and w[i] == triple[0][2]:\n",
    "                triple_start_pos = i-2\n",
    "                triple_end_pos = i\n",
    "        \n",
    "        from_pos = min(entity_pos, triple_start_pos)\n",
    "        to_pos = max(entity_pos, triple_end_pos)\n",
    "\n",
    "        relevant_walks.append(w[from_pos:to_pos+1])\n",
    "\n",
    "\n",
    "\n",
    "relevant_triples = iw.walks_as_triples(relevant_walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "labels = {}\n",
    "\n",
    "min_edge_width = 0.25\n",
    "max_edge_width = 8\n",
    "max_score = max([abs(exp[1]) for exp in top_triples])\n",
    "\n",
    "uri_trimmer = lambda uri: uri.split(\"/\")[-1].split(\"#\")[-1].replace(\":\", \"_\")\n",
    "triple_trimmer = lambda triple: (uri_trimmer(triple[0]), uri_trimmer(triple[1]), uri_trimmer(triple[2]))\n",
    "\n",
    "for rt in relevant_triples:\n",
    "    score = next((exp[1] for exp in top_triples if rt==exp[0]), None)\n",
    "    \n",
    "    if score:\n",
    "        # score *= -1\n",
    "        color = \"green\" if score > 0 else \"red\"\n",
    "        width = max(abs(score)/max_score*max_edge_width, min_edge_width)\n",
    "    else:\n",
    "        color = \"blue\"\n",
    "        width = min_edge_width\n",
    "        #print(rt, score)\n",
    "\n",
    "    rt = triple_trimmer(rt)\n",
    "    edge = (rt[0], rt[2], {\"width\": width, \"color\": color})\n",
    "    \n",
    "    label = rt[1]\n",
    "    if score: label += f\" ({round(width/max_edge_width, 2)})\"\n",
    "\n",
    "    edges.append(edge)\n",
    "    \n",
    "    labels[(edge[0],edge[1])] = label\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(edges)\n",
    "#pos = nx.spring_layout(G, k=1.25/sqrt(len(G.nodes())), seed=42, iterations=35)\n",
    "pos = nx.nx_pydot.graphviz_layout(G, prog=\"neato\")\n",
    "plt.figure(figsize=(18,18))\n",
    "nx.draw(\n",
    "    G, pos,\n",
    "    node_size=500, node_color='lightblue', alpha=0.9,\n",
    "    labels={node: node for node in G.nodes()},\n",
    "    arrowstyle='->',\n",
    "    width=[G[u][v]['width'] for u,v in G.edges()],\n",
    "    edge_color=[G[u][v]['color'] for u,v in G.edges()],\n",
    ")\n",
    "\n",
    "\n",
    "_ = nx.draw_networkx_edge_labels(\n",
    "    G, pos,\n",
    "    edge_labels=labels,\n",
    "    font_color='black',\n",
    ")\n",
    "\n",
    "# plt.savefig(\"green_lantern.pdf\")\n",
    "\n",
    "x = nx.nx_pydot.to_pydot(G)\n",
    "\n",
    "# Bug: node labels with :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_explanations():\n",
    "    exp_list = explanation.as_list()[:25]\n",
    "\n",
    "    for exp in exp_list:\n",
    "        triple = exp[0]\n",
    "        if \"_\" in triple: continue\n",
    "        modified_walks = explainer.get_perturbed_walks(explained_entity_uri, added_triples=[], removed_triples=[triple])\n",
    "        embedding = explainer.get_perturbed_embedding(explained_entity_uri, modified_walks)\n",
    "        new_prediction = clf.predict_proba([embedding])\n",
    "        print( (new_prediction[0][1] - prediction[0][1]) / exp[1], triple )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Explained Change / Real Change\n",
    "#\n",
    "# +0.5 / +0.5 \n",
    "# +\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_explanations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "nt = Network('500px', '500px')\n",
    "nt.from_nx(G)\n",
    "nt.show(\"nx.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_old = explainer.old_transformer.embedder._model.wv\n",
    "wv_new = explainer.transformer.embedder._model.wv\n",
    "def embedding(URI, new):\n",
    "    wv = wv_new if new else wv_old\n",
    "    return wv.get_vector(URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_old.most_similar(explainer.new_embeddings[150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the movie embedding itself drifted\n",
    "all(embedding(explained_entity_uri, new=True) == embedding(explained_entity_uri, new=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for walk in explainer.indexedWalks.walks(e):\n",
    "    for step in walk:\n",
    "        if not step in changed and not step == e:\n",
    "            print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "# Reduce the dimensions of entity embeddings to represent them in a 2D plane.\n",
    "X_tsne = TSNE(random_state=42).fit_transform([wv_old[explained_entity_uri]] + explainer.new_embeddings)\n",
    "\n",
    "labels = explainer.old_transformer._entities + list(range(len(explainer.new_embeddings)))\n",
    "labels = [explained_entity_uri] + list(range(len(explainer.new_embeddings)))\n",
    "\n",
    "#colors = list(map(lambda e: \"#00ff00\" if movieTest[movieTest.DBpedia_URI==e].iloc[0].label == \"good\" else \"#ff0000\", transformer._entities[1600:]))\n",
    "#sizes = list(map(lambda e: abs(50-movieTest[movieTest.DBpedia_URI==e].iloc[0].rating)**2, transformer._entities[1600:]))\n",
    "sizes = list(map(lambda l: 100**2 if l == explained_entity_uri or l == 0 else 25**2, labels))\n",
    "\n",
    "# Ploy the embeddings of entities in a 2D plane, annotating them.\n",
    "f = plt.figure(figsize=(200, 80))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], s=sizes)#, c=colors, s=sizes)\n",
    "\n",
    "for x, y, t in zip(X_tsne[:, 0], X_tsne[:, 1], labels):\n",
    "    if type(t) == str and \"http\" in t: t = t.split(\"/\")[-1]\n",
    "    plt.annotate(t, (x, y))\n",
    "\n",
    "# Display the graph with a title, removing the axes for better readability.\n",
    "plt.title(\"pyRDF2Vec\", fontsize=4)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "#f.savefig(\"figure.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_triples = len(explainer.indexed_walks.walks_as_triples(explainer.indexed_walks.walks(explained_entity_uri)))\n",
    "unique_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keanu = embedding(\"http://dbpedia.org/resource/Keanu_Reeves\", new=True)\n",
    "matrix = embedding(\"http://dbpedia.org/resource/The_Matrix\", new=True)\n",
    "wv_new.most_similar(matrix-keanu, topn=len(wv_new))[0]\n",
    "wv_new.most_similar(positive=[\"http://dbpedia.org/resource/The_Matrix\"], negative=[\"http://dbpedia.org/resource/Keanu_Reeves\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_search = lambda query: [(i,x) for i,x in enumerate(movies) if query.lower() in x.lower()]\n",
    "movie_search(\"star\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieFull[200:220]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def xm(i): \n",
    "    print(i)\n",
    "\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    executor.map(xm, [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(155):\n",
    "\n",
    "    reload(lime.lime_rdf)\n",
    "    from lime.lime_rdf import LimeRdfExplainer\n",
    "\n",
    "    with open(os.path.join(movieLocation, \"transformers\", \"rdf2vec_transformer_cbow_200\"), \"rb\") as file:\n",
    "        rdf2vec_transformer = pickle.load(file)\n",
    "\n",
    "    with open(os.path.join(movieLocation, \"classifiers\", \"svc_100_cbow_200\"), \"rb\") as file:\n",
    "        clf = pickle.load(file)\n",
    "\n",
    "    explainer = LimeRdfExplainer(\n",
    "        transformer=rdf2vec_transformer, \n",
    "        entities=movies,\n",
    "        class_names=clf.classes_,\n",
    "        kernel=None,\n",
    "        kernel_width=25,\n",
    "        verbose=False,\n",
    "        feature_selection=\"auto\",\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "\n",
    "    explained_entity_id = i  # 0-400 -> test data\n",
    "    explained_entity_uri = movies[explained_entity_id]\n",
    "    prediction = clf.predict_proba([rdf2vec_transformer._embeddings[explained_entity_id]])\n",
    "\n",
    "\n",
    "    print(\"Explaining\", explained_entity_uri)\n",
    "    print(\"Original prediction:\", prediction, \" / \".join(clf.classes_))\n",
    "    print(\"True class:\", movieFull.iloc[explained_entity_id].label)\n",
    "\n",
    "    data, probabilities, distances, explanation = explainer.explain_instance(\n",
    "        entity=explained_entity_uri, \n",
    "        classifier_fn=clf.predict_proba,\n",
    "        num_features=50,\n",
    "        num_samples=5000,\n",
    "        allow_triple_addition=False,\n",
    "        allow_triple_subtraction=True,\n",
    "        max_changed_triples=20,\n",
    "        change_count_fixed=True,\n",
    "        use_w2v_freeze=True,\n",
    "        center_correction=False,\n",
    "        single_run=False,\n",
    "        train_with_all=False,\n",
    "        distance_metric=\"cosine\",\n",
    "        model_regressor=None,\n",
    "        short_uris=False\n",
    "    )\n",
    "\n",
    "    with open(f\"explanation-{i}\", \"wb\") as f:\n",
    "        pickle.dump([explanation, probabilities], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce5a4af9a72a016c4887bc4fbd6c73d917f9c9f5d4de7f9550be99bde2950c61"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rdflime-util-cLuY5sAo-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
